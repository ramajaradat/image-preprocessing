{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "human=cv2.imread(\"./data_2/human.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_copy=human.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 2&3 use to dont get error when we close the output window\n",
    "cv2.imshow('human',human)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 768, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask\n",
    "mask=np.zeros(human.shape[:2], np.uint8) # mask=np.zeros(human.shape[:-1]) that same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat dummy array for fetures using\n",
    "bgd=np.zeros((1,65),np.float64) # creat background\n",
    "fgd=np.zeros((1,65),np.float64) # creat front background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to select region using openv => by using  selectro function \n",
    "# thes function help me to find thw w&h for the obj which i need\n",
    "rect=cv2.selectROI(human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('human',rect)\n",
    "cv2.waitKey(10)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h=rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[34, 60, 67],\n",
       "        [31, 57, 64],\n",
       "        [29, 54, 64],\n",
       "        ...,\n",
       "        [30, 56, 63],\n",
       "        [32, 55, 63],\n",
       "        [32, 55, 63]],\n",
       "\n",
       "       [[34, 60, 67],\n",
       "        [34, 60, 67],\n",
       "        [33, 58, 68],\n",
       "        ...,\n",
       "        [28, 54, 61],\n",
       "        [32, 54, 65],\n",
       "        [32, 55, 63]],\n",
       "\n",
       "       [[36, 61, 71],\n",
       "        [35, 60, 70],\n",
       "        [33, 58, 68],\n",
       "        ...,\n",
       "        [28, 53, 63],\n",
       "        [33, 55, 67],\n",
       "        [34, 56, 67]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[37, 55, 38],\n",
       "        [38, 56, 39],\n",
       "        [35, 54, 37],\n",
       "        ...,\n",
       "        [54, 99, 82],\n",
       "        [51, 95, 78],\n",
       "        [46, 90, 73]],\n",
       "\n",
       "       [[36, 55, 36],\n",
       "        [37, 56, 37],\n",
       "        [35, 55, 36],\n",
       "        ...,\n",
       "        [53, 98, 82],\n",
       "        [50, 95, 79],\n",
       "        [46, 91, 75]],\n",
       "\n",
       "       [[35, 54, 35],\n",
       "        [37, 56, 37],\n",
       "        [35, 55, 36],\n",
       "        ...,\n",
       "        [50, 97, 81],\n",
       "        [48, 93, 77],\n",
       "        [43, 88, 72]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.rectangle(human_copy,(x,y),(x+w,y+h),(0,0,255),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"Human_ROI.png\",human_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " array([[1.90178714e-01, 1.50806323e-01, 1.19742364e-01, 2.07970279e-01,\n",
       "         3.31302320e-01, 3.49409454e+01, 8.30500442e+01, 6.43681204e+01,\n",
       "         5.40826401e+01, 9.87794752e+01, 1.03235858e+02, 2.88135819e+01,\n",
       "         5.01946329e+01, 4.52922113e+01, 3.67221453e+01, 6.38394593e+01,\n",
       "         7.91742778e+01, 4.50461372e+01, 1.00305490e+02, 8.18609000e+01,\n",
       "         6.17882248e+01, 4.03773575e+01, 3.97164064e+01, 4.03773575e+01,\n",
       "         6.18321307e+01, 6.11120555e+01, 3.97164064e+01, 6.11120555e+01,\n",
       "         6.94877614e+01, 2.34287260e+02, 2.36728369e+02, 2.27525430e+02,\n",
       "         2.36728369e+02, 3.15241432e+02, 1.76477313e+02, 2.27525430e+02,\n",
       "         1.76477313e+02, 3.83222126e+02, 7.44651650e+01, 7.54763145e+01,\n",
       "         5.25574380e+01, 7.54763145e+01, 1.52663597e+02, 5.05957679e+01,\n",
       "         5.25574380e+01, 5.05957679e+01, 1.06788818e+02, 4.81349271e+01,\n",
       "         5.68871186e+01, 6.89011131e+01, 5.68871186e+01, 9.31739635e+01,\n",
       "         1.12419761e+02, 6.89011131e+01, 1.12419761e+02, 1.58516096e+02,\n",
       "         2.70055824e+01, 2.33402262e+01, 2.50183285e+01, 2.33402262e+01,\n",
       "         2.97449248e+01, 2.79680167e+01, 2.50183285e+01, 2.79680167e+01,\n",
       "         3.28894612e+01]]),\n",
       " array([[ 6.39773393e-03,  5.79654229e-01,  1.90222700e-01,\n",
       "          1.18162727e-01,  1.05562610e-01,  3.96145038e+01,\n",
       "          6.41793893e+01,  7.97557252e+01,  5.03932935e+01,\n",
       "          5.55869492e+01,  1.96729800e+02,  1.25956354e+01,\n",
       "          1.48596919e+01,  7.59680359e+01,  8.57125439e+01,\n",
       "          1.01467039e+02,  1.83907626e+02,  3.61917650e+01,\n",
       "          4.28605135e+01,  1.37355771e+02,  2.87702538e+02,\n",
       "          2.69752360e+02,  2.21066138e+02,  2.69752360e+02,\n",
       "          3.84368583e+02,  2.42662141e+02,  2.21066138e+02,\n",
       "          2.42662141e+02,  2.28138803e+02,  1.53954597e+02,\n",
       "          1.40007045e+02,  1.89331013e+02,  1.40007045e+02,\n",
       "          1.35945279e+02,  1.81294072e+02,  1.89331013e+02,\n",
       "          1.81294072e+02,  3.13069801e+02,  1.09310430e+02,\n",
       "          7.87856264e+01,  5.10314908e+01,  7.87856264e+01,\n",
       "          9.40554099e+01,  5.41668887e+01,  5.10314908e+01,\n",
       "          5.41668887e+01,  7.01450711e+02,  7.75028962e+02,\n",
       "          9.08610178e+02,  3.13098059e+02,  9.08610178e+02,\n",
       "          1.15669859e+03,  3.53300840e+02,  3.13098059e+02,\n",
       "          3.53300840e+02,  5.01820564e+02,  1.74943333e+02,\n",
       "          2.08471579e+02, -9.93359578e+00,  2.08471579e+02,\n",
       "          2.96176472e+02, -4.38770460e+01, -9.93359578e+00,\n",
       "         -4.38770460e+01,  2.10282865e+02]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algorithm grabcut we cut original image according the rect image \n",
    "#5==num of iteration\n",
    "cv2.grabCut(human,mask,rect,bgd,fgd,5,cv2.GC_INIT_WITH_RECT)\n",
    "# this fun return 3 valuse one respect bgd,one respect values we find by ROI and the last one respect human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.min(),mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1450926946622932"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the most of valuse is go to be 1.14 ==>white so background 0 mask=1 ,rect=2 \n",
    "mask.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creat mask2 we get 3values from grabcut 0,1,2\n",
    "mask2 = np.where((mask == 2 ) | (mask == 0), 0, 1).astype(\"uint8\")#last 0.1 is refer to white if true else black \n",
    "cv2.imwrite(\"mask.png\", mask*80)\n",
    "cv2.imwrite(\"mask2.png\", mask2*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.ndim,mask2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference on original data (image),mask2 return just human so we take it\n",
    "human = human * mask2[:,:,np.newaxis]\n",
    "cv2.imwrite(\"result.png\", human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_cascae_car=\"./data_2//xml/haarcascade_car.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_class = cv2.CascadeClassifier (haar_cascae_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cars video using fun in cv => vieo capture\n",
    "cap= cv2.VideoCapture(\"./data_2/cars.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "while cap.isOpened():\n",
    "    count+=1\n",
    "    if count  % 20==0:# every 20 milliseconde        \n",
    "        ret,frame=cap.read()        \n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) # convert each frame to gray\n",
    "        cars=car_class.detectMultiScale(gray, 1.1, 3)#calssification , detect for every frame\n",
    "        \n",
    "        for(x,y,w,h) in cars:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)          \n",
    "            cv2.putText(frame,\"car\",(x,y),cv2.FONT_HERSHEY_SIMPLEX ,0.5, (0,0,255))  # fun control the put text like the type of font the coordinate ,...etc // note we take this argument from document\n",
    "        cv2.imshow(\"cars\",frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):  # waitKey    \n",
    "        break \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
